{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd source_QMaxViT-Unet+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import importlib\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.functional import one_hot\n",
    "from time import strftime\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.datasets import ACDCDataset_Edge, MSCMRDataSets_Edge\n",
    "from dataloaders.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_acdc = ACDCDataset_Edge(\n",
    "    base_dir=\"/teamspace/studios/this_studio/ACDC_augmentated_onlyEdge\",\n",
    "    split=\"train\",\n",
    "    transform=transforms.Compose([RandomGenerator([256,256], is_edge_mask=True)]),\n",
    "    fold=\"fold1\",\n",
    "    sup_type=\"label\",\n",
    "    is_edge_mask=True,\n",
    ")\n",
    "\n",
    "val_set_acdc = ACDCDataset_Edge(\n",
    "    base_dir=\"/teamspace/studios/this_studio/ACDC_preprocessed\",\n",
    "    split='val',\n",
    "    transform=None,\n",
    "    fold=\"fold1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set_acdc[0]\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(sample['image'].squeeze(), cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sample['label'])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sample['edge_mask'].squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_mscmr = MSCMRDataSets_Edge(\n",
    "    base_dir=\"/teamspace/studios/this_studio/MSCMR_augmentated\",\n",
    "    split=\"train\",\n",
    "    transform=transforms.Compose([RandomGenerator([256,256], is_edge_mask=True)]),\n",
    "    fold=\"MAAGfold\",\n",
    "    sup_type=\"label\",\n",
    "    train_dir=\"/MSCMR_training_slices\", \n",
    "    is_edge_mask=True,\n",
    ")\n",
    "\n",
    "val_set_mscmr = MSCMRDataSets_Edge(\n",
    "    base_dir=\"/teamspace/studios/this_studio/MSCMR_preprocessed\",\n",
    "    split='val',\n",
    "    transform=None,\n",
    "    val_dir=\"/MSCMR_testing_volumes\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_set_mscmr[0]\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(sample['image'].squeeze(), cmap='gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sample['label'])\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sample['edge_mask'].squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.qemaxvit_unet import QEMaxViT_Unet\n",
    "model = QEMaxViT_Unet(num_classes=4, backbone_pretrained_pth=\"/teamspace/studios/this_studio/MIST/pretrained_pth/maxvit/maxxvit_rmlp_small_rw_256_sw-37e217ff.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "for i in model(torch.rand(1,3,256,256)):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='0, 1'\n",
    "\n",
    "deterministic = 1\n",
    "if not deterministic:\n",
    "    cudnn.benchmark = True\n",
    "    cudnn.deterministic = False\n",
    "else:\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "\n",
    "seed = 444\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(seed + worker_id)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "batch_size = 12\n",
    "trainloader = DataLoader(\n",
    "    train_set_acdc,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=12,\n",
    "    pin_memory=True,\n",
    "    worker_init_fn=worker_init_fn)\n",
    "\n",
    "valloader = DataLoader(\n",
    "    val_set_acdc,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.losses import pDLoss\n",
    "from utils import pyutils\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch import optim\n",
    "\n",
    "num_classes = 4\n",
    "ce_loss = CrossEntropyLoss(ignore_index=4)\n",
    "dice_loss = pDLoss(num_classes, ignore_index=4)\n",
    "edge_loss_function = nn.MSELoss()\n",
    "avg_meter = pyutils.AverageMeter('loss')\n",
    "\n",
    "best_performance = 0.0\n",
    "best_epoch = 0\n",
    "iter_num = 0\n",
    "max_epoches = 200\n",
    "\n",
    "max_iterations = max_epoches * len(trainloader)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100 * len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from medpy import metric\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "\n",
    "def calculate_metric_percase(pred, gt):\n",
    "    pred[pred > 0] = 1\n",
    "    gt[gt > 0] = 1\n",
    "    if gt.sum() == 0 and pred.sum() == 0:\n",
    "        return np.nan, np.nan\n",
    "    elif gt.sum() == 0 and pred.sum() > 0:\n",
    "        return 0, 0\n",
    "    elif gt.sum() > 0:\n",
    "        dice = metric.binary.dc(pred, gt)\n",
    "        if pred.sum() == 0:\n",
    "            hd95 = np.nan\n",
    "        else:\n",
    "            hd95 = metric.binary.hd95(pred, gt)\n",
    "        return dice, hd95\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_single_volume_for_training(image, label, net, classes, patch_size=[256, 256]):\n",
    "    image, label = image.squeeze(0).cpu().detach(\n",
    "    ).numpy(), label.squeeze(0).cpu().detach().numpy()\n",
    "    if len(image.shape) == 3:\n",
    "        prediction_1 = np.zeros_like(label)\n",
    "        prediction_2 = np.zeros_like(label)\n",
    "        for ind in range(image.shape[0]):\n",
    "            slice = image[ind, :, :]\n",
    "            x, y = slice.shape[0], slice.shape[1]\n",
    "            slice = zoom(\n",
    "                slice, (patch_size[0] / x, patch_size[1] / y), order=0)\n",
    "            input = torch.from_numpy(slice).unsqueeze(\n",
    "                0).unsqueeze(0).float().cuda()\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                P1,P2,_ = net(input)\n",
    "#                 val_outputs = 0.0\n",
    "#                 for idx in range(len(P)):\n",
    "#                     val_outputs += P[idx]\n",
    "\n",
    "                iout_soft1 = torch.softmax(P1, dim=1)\n",
    "                iout_soft2 = torch.softmax(P2, dim=1)\n",
    "                #iout_soft = torch.softmax(P, dim=1)\n",
    "                out_2 = torch.argmax((iout_soft2+iout_soft1), dim=1).squeeze(0)\n",
    "                out_1 = torch.argmax(iout_soft1, dim=1).squeeze(0)\n",
    "                out_2 = out_2.cpu().detach().numpy()\n",
    "                out_1 = out_1.cpu().detach().numpy()\n",
    "                \n",
    "                pred_1 = zoom(\n",
    "                    out_1, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "                prediction_1[ind] = pred_1\n",
    "                \n",
    "                pred_2 = zoom(\n",
    "                    out_2, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "                prediction_2[ind] = pred_2\n",
    "                \n",
    "    else:\n",
    "        input = torch.from_numpy(image).float().cuda()\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            P = net(input)\n",
    "            # val_outputs = 0.0\n",
    "            # for idx in range(len(P)):\n",
    "            #     val_outputs += P[idx]\n",
    "\n",
    "            iout_soft = torch.softmax(P, dim=1)\n",
    "            out = torch.argmax(iout_soft, dim=1).squeeze(0)\n",
    "            prediction = out.cpu().detach().numpy()\n",
    "\n",
    "    metric_list_one = []\n",
    "    metric_list_two = []\n",
    "    for i in range(1, classes):\n",
    "        metric_list_one.append(calculate_metric_percase(\n",
    "            prediction_1 == i, label == i))\n",
    "        metric_list_two.append(calculate_metric_percase(\n",
    "            prediction_2 == i, label == i))\n",
    "    return metric_list_one, metric_list_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress_table import ProgressTable\n",
    "table = ProgressTable(\n",
    "    pbar_embedded=False,  # Do not use embedded pbar\n",
    "    pbar_style=\"angled alt red blue\",\n",
    "    interactive=1,\n",
    "    pbar_show_eta=True,\n",
    ")\n",
    "table.add_columns(\"epoch\")\n",
    "table.add_columns(\"train_loss\")\n",
    "table.add_columns(\"dice_one\")\n",
    "table.add_columns(\"dice_two\")\n",
    "table.add_columns(\"hd95_one\")\n",
    "table.add_columns(\"hd95_two\")\n",
    "table.add_columns(\"best_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add early stopping if necessary\n",
    "max_epochs_without_improvement = 50\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "best_model_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "iter_num = 0\n",
    "#for ep in range(max_epoches):\n",
    "for ep in table(max_epoches, show_throughput=False, show_eta=True):\n",
    "    table[\"epoch\"] = ep\n",
    "    #for iter, sampled_batch in enumerate(trainloader):\n",
    "    for sampled_batch in table(trainloader, description=\"train epoch\"):\n",
    "        img, label, groundtruth_edge = sampled_batch['image'], sampled_batch['label'], sampled_batch['edge_mask']\n",
    "        img, label, groundtruth_edge = img.to(device), label.to(device), groundtruth_edge.to(device)\n",
    "\n",
    "        output_main, output_aux, edge_map = model(img)\n",
    "        \n",
    "        \n",
    "        outputs_soft1 = torch.softmax(output_main, dim=1)\n",
    "        outputs_soft2 = torch.softmax(output_aux, dim=1)\n",
    "        \n",
    "        beta = random.random() + 1e-10\n",
    "\n",
    "        pseudo_supervision = torch.argmax(\n",
    "                (beta * outputs_soft1.detach() + (1.0-beta) * outputs_soft2.detach()), dim=1, keepdim=False)\n",
    "\n",
    "        loss_pse_sup = 0.5 * (dice_loss(outputs_soft1, pseudo_supervision.unsqueeze(\n",
    "                1)) + dice_loss(outputs_soft2, pseudo_supervision.unsqueeze(1)))\n",
    "\n",
    "            \n",
    "        loss_ce1 = ce_loss(output_main, label[:].long())\n",
    "        loss_ce2 = ce_loss(output_aux, label[:].long())\n",
    "        loss_ce = 0.5 * (loss_ce1 + loss_ce2)  # <--- loss ce\n",
    "        \n",
    "        edge_loss = edge_loss_function(edge_map, groundtruth_edge)  # <--- loss edge\n",
    "\n",
    "        loss = loss_ce + 0.5 * loss_pse_sup + 0.2 * edge_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_meter.add({'loss': loss.item()})\n",
    "        \n",
    "        scheduler.step()\n",
    "        iter_num += 1\n",
    "    else:\n",
    "        table.update(\"train_loss\", avg_meter.get('loss'), color=\"blue\")\n",
    "\n",
    "        model.eval()\n",
    "        metric_list_one = []\n",
    "        metric_list_two = []\n",
    "        for sampled_batch in table(valloader, description=\"valid epoch\"):\n",
    "            metric_i_one, metric_i_two = test_single_volume_for_training(\n",
    "                sampled_batch[\"image\"], sampled_batch[\"label\"], model, classes=num_classes)\n",
    "            metric_list_one.append(metric_i_one)\n",
    "            metric_list_two.append(metric_i_two)\n",
    "        \n",
    "        metric_list_one = np.nanmean(np.array(metric_list_one), axis=0)\n",
    "        metric_list_two = np.nanmean(np.array(metric_list_two), axis=0)\n",
    "            \n",
    "                 \n",
    "        performance = None\n",
    "        mean_hd95 = None\n",
    "        \n",
    "        performance_one = np.mean(metric_list_one, axis=0)[0]\n",
    "        mean_hd95_one = np.mean(metric_list_one, axis=0)[1]\n",
    "        \n",
    "        performance_two = np.mean(metric_list_two, axis=0)[0]\n",
    "        mean_hd95_two = np.mean(metric_list_two, axis=0)[1]\n",
    "\n",
    "       \n",
    "        if performance_one > best_performance or performance_two > best_performance:\n",
    "            table[\"best_model\"] = \"✅\"\n",
    "            if performance_one > performance_two:\n",
    "                performance = performance_one\n",
    "                mean_hd95 = mean_hd95_one\n",
    "            else:\n",
    "                performance = performance_two\n",
    "                mean_hd95 = mean_hd95_two\n",
    "            \n",
    "            best_performance = performance\n",
    "            best_epoch = ep\n",
    "            epochs_without_improvement = 0\n",
    "            save_best = \"/teamspace/studios/this_studio/acdc_bestmodel/Qmaxvitunet_aaa_test.pth\"\n",
    "            torch.save(model.state_dict(), save_best)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= max_epochs_without_improvement:\n",
    "                print(f'Early stopping at epoch {ep} as validation performance did not improve.')\n",
    "                break\n",
    "       \n",
    "        table.update(\"dice_one\", performance_one, color=\"green\")\n",
    "        table.update(\"hd95_one\", mean_hd95_one, color=\"green\")\n",
    "        table.update(\"dice_two\", performance_two, color=\"blue\")\n",
    "        table.update(\"hd95_two\", mean_hd95_two, color=\"blue\")\n",
    "        model.train()\n",
    "        avg_meter.pop()\n",
    "        table.next_row()\n",
    "print('best model in epoch %5d  mean_dice : %.4f' % (best_epoch, best_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_acdc = ACDCDataset_Edge(\n",
    "    base_dir=\"/teamspace/studios/this_studio/ACDC_preprocessed\",\n",
    "    split='test',\n",
    "    fold=\"MAAGfold\",\n",
    "    transform=None,\n",
    ")\n",
    "\n",
    "testloader_acdc = DataLoader(\n",
    "    test_set_acdc,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference = QEMaxViT_Unet(num_classes=4, backbone_pretrained_pth=\"/teamspace/studios/this_studio/MIST/pretrained_pth/maxvit/maxxvit_rmlp_small_rw_256_sw-37e217ff.pth\")\n",
    "model_inference.load_state_dict(torch.load(\"/teamspace/studios/this_studio/acdc_bestmodel/Qmaxvitunet_final_fold4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "model_inference.to(device)\n",
    "model_inference.eval()\n",
    "# model.eval()\n",
    "metric_list_one = []\n",
    "metric_list_two = []\n",
    "for i_batch, sampled_batch in enumerate(tqdm(testloader_acdc)):\n",
    "    metric_i_one, metric_i_two = test_single_volume_for_training(\n",
    "        sampled_batch[\"image\"], sampled_batch[\"label\"], model_inference, classes=4)\n",
    "    metric_list_one.append(metric_i_one)\n",
    "    metric_list_two.append(metric_i_two)\n",
    "\n",
    "metric_list_one = np.nanmean(np.array(metric_list_one), axis=0)\n",
    "metric_list_two = np.nanmean(np.array(metric_list_two), axis=0)\n",
    "\n",
    "df_one = pd.DataFrame(metric_list_one, columns=['Dice', 'HD95'], index=['RV', 'Myo', 'LV'])\n",
    "df_two = pd.DataFrame(metric_list_two, columns=['Dice', 'HD95'], index=['RV', 'Myo', 'LV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_one.round(3))\n",
    "print(df_two.round(3))\n",
    "df_one['Dice'].mean(), df_one['HD95'].mean(), df_two['Dice'].mean(), df_two['HD95'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def inference(image, label, net, classes, patch_size=[256, 256]):\n",
    "    image, label = image.squeeze(0).cpu().detach(\n",
    "    ).numpy(), label.squeeze(0).cpu().detach().numpy()\n",
    "    if len(image.shape) == 3:\n",
    "        prediction_1 = np.zeros_like(label)\n",
    "        prediction_2 = np.zeros_like(label)\n",
    "        for ind in range(image.shape[0]):\n",
    "            slice = image[ind, :, :]\n",
    "            x, y = slice.shape[0], slice.shape[1]\n",
    "            slice = zoom(\n",
    "                slice, (patch_size[0] / x, patch_size[1] / y), order=0)\n",
    "            input = torch.from_numpy(slice).unsqueeze(\n",
    "                0).unsqueeze(0).float().cuda()\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                P1,P2,_ = net(input)\n",
    "#                 val_outputs = 0.0\n",
    "#                 for idx in range(len(P)):\n",
    "#                     val_outputs += P[idx]\n",
    "\n",
    "                iout_soft1 = torch.softmax(P1, dim=1)\n",
    "                iout_soft2 = torch.softmax(P2, dim=1)\n",
    "                #iout_soft = torch.softmax(P, dim=1)\n",
    "                out_2 = torch.argmax(iout_soft2, dim=1).squeeze(0)\n",
    "                out_1 = torch.argmax(iout_soft1, dim=1).squeeze(0)\n",
    "                out_2 = out_2.cpu().detach().numpy()\n",
    "                out_1 = out_1.cpu().detach().numpy()\n",
    "                \n",
    "                pred_1 = zoom(\n",
    "                    out_1, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "                prediction_1[ind] = pred_1\n",
    "                \n",
    "                pred_2 = zoom(\n",
    "                    out_2, (x / patch_size[0], y / patch_size[1]), order=0)\n",
    "                prediction_2[ind] = pred_2\n",
    "                \n",
    "    return prediction_1, prediction_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference.cuda()\n",
    "model_inference.eval()\n",
    "for i_batch, sampled_batch in enumerate(tqdm(testloader_acdc)):\n",
    "    if i_batch == 200:\n",
    "        break\n",
    "    prediction_1, prediction_2 = inference(sampled_batch[\"image\"], sampled_batch[\"label\"], model_inference, 4)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(sampled_batch['image'].squeeze(0).cpu().numpy()[2], cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(sampled_batch['label'].squeeze(0).cpu().numpy()[2])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(prediction_1[2])\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(prediction_2[2])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
